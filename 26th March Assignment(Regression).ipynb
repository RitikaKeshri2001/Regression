{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "672d87bf-f701-4583-9e50-5f38aa270303",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "    Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables.\n",
    "    Whereas linear regress only has one independent variable impacting the slope of the relationship, multiple regression incorporates multiple independent variables.\n",
    "\n",
    "    Linear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables.\n",
    "    There are two types of Linear regression: Simple linear regression and Multiple linear regression.\n",
    "\n",
    "    Difference between Simple linear regression and multiple linear regression are:\n",
    "    1. Simple Linear Regression is a type of Regression algorithms that models the relationship between a dependent variable and a single independent variable. whereas, Multiple Linear Regression is one of the important regression algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable.\n",
    "\n",
    "    2. The Simple Linear Regression model can be represented using the equation: y= a0 + a1x.\n",
    "    Where, a0= It is the intercept of the Regression line (can be obtained putting x=0), a1= It is the slope of the regression line, which tells whether the line is increasing or decreasing.\n",
    "\n",
    "    whereas, The Multiple Linear Regression model can be represented using the equation: y = a0 + a1*x1 + a2*x2 +...+ an*xn\n",
    "    Y= Output/Response variable; a0, a1, a2, a3 , an....= Coefficients of the model; x1, x2, x3, x4,...= Various Independent/feature variable\n",
    "\n",
    "    3. Simple linear regression has only one x and one y variable. whereas, Multiple linear regression has one y and two or more x variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd6ff2-ac27-4be5-9f3a-5666fb87265c",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "    1. Linear relationship between the features and target:\n",
    "        Linear regression assumes the linear relationship between the dependent and independent variables.\n",
    "\n",
    "    2. Independence: \n",
    "        The observations should be independent of each other. This means that the value of one observation should not depend on the value of another observation.\n",
    "\n",
    "    3. Homoscedasticity :\n",
    "        Homoscedasticity is a situation when the error term is the same for all the values of independent variables. With homoscedasticity, there should be no clear pattern distribution of data in the scatter plot. The variance of the errors should be constant across all levels of the independent variable.\n",
    "\n",
    "    4. Normal distribution of error terms:\n",
    "        Linear regression assumes that the error term should follow the normal distribution pattern. If error terms are not normally distributed, then confidence intervals will become either too wide or too narrow, which may cause difficulties in finding coefficients.It can be checked using the q-q plot. If the plot shows a straight line without any deviation, which means the error is normally distributed.\n",
    "\n",
    "    5. No multicollinearity: \n",
    "        Multicollinearity means high-correlation between the independent variables. Due to multicollinearity, it may difficult to find the true relationship between the predictors and target variables. There should be no perfect linear relationship between the independent variables. This means that the independent variables should not be highly correlated with each other.\n",
    "\n",
    "    To check whether these assumptions hold in a given dataset:\n",
    "    1. Linear relationship between the features and target: We can plot a scatter plot of dependent variable v/s independent variable to visualize the relationship. If the plot show a roughly straight line, then the relationship is linear.\n",
    "\n",
    "    2. Independence: \n",
    "    Difficult to check directly, but can be checked by ensuring that there are no confounding factors that could influence the observations.\n",
    "\n",
    "    3. Homoscedasticity: \n",
    "    To check Homoscedasticity we plot a graph of residuals against the predicted values. If the plot shows a funnel shape, this indicates that the assumption is violated.\n",
    "\n",
    "    4. Normal distribution of error terms:\n",
    "    To check the Normal distribution of error terms we plot a histogram or a Q-Q plot of the residuals. If the plot shows a significant deviation from normality, this indicates that the assumption is violated.\n",
    "\n",
    "    5. No multicollinearity:\n",
    "    If the correlation coefficient or variance inflation factor(VIF) is greater than a certain threshold, this indicates that the assumption is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c505a-6d0d-44cd-8621-95d2c4f4bca9",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "    A real-world scenario:\n",
    "\n",
    "    Suppose we want to predict the salary of an employee based on their years of experience. We collect data on the years of experience. We collect data on the years of experience and salaries of employees in a company and formula for a linear regression model is: Salary = 30000 + 5000 * years of experience\n",
    "\n",
    "    Interpret the slope: If the years of experience is goes up by 1 yearthen we predict the Salary will increase by 5000.\n",
    "\n",
    "    Interpret the intercept: If the years of experience is 0, then we predict the Salary of the employee will be 30000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429b6d2-aec1-48e8-a190-53e8558a49cf",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "    Gradient descent was initially discovered by \"Augustin-Louis Cauchy\" in mid of 18th century. Gradient Descent is defined as one of the most commonly used iterative optimization algorithms of machine learning to train the machine learning and deep learning models. It helps in finding the local minimum of a function. Optimization is the task of minimizing the cost function parameterized by the model's parameters. The main objective of gradient descent is to minimize the convex function using iteration of parameter updates. Gradient descent is a numerical optimization algorithm used to minimize the cost function in machine learning. \n",
    "\n",
    "    It is used in Machine learning as:\n",
    "    It initially starts with an initial guess for the parameters and calculates the gradient of the cost function with respect to each parameter. It then updates the parameters by subtracting the product of the gradient and a learning rate from the current parameter values. The learning rate determines the step size of the updates and controls how quickly the algorithm converges to the minimum. If the learning rate is too high, the algorithm may overshoot the minimum, and if it is too low, it may take a long time to converge. The process is repeated until the cost function reach to a minimum. At each iteration, the algorithm calculates the gradient and updates the parameters until the cost function is minimized. \n",
    "\n",
    "    The gradient descent algorithm is used to find the optimal set of parameters that minimize the loss function. The performance of the model depends on the choice of the cost function and the learning rate. The use of gradient descent has revolutionized machine learning and has enabled the development of complex models that can learn from large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95733727-c957-41a5-bee7-5e08d80325b6",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "    Multiple Linear Regression is one of the important regression algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable. There may be various cases in which the response variable is affected by more than one predictor variable; for such cases, the Multiple Linear Regression algorithm is used. Multiple Linear Regression is an extension of Simple Linear regression as it takes more than one predictor variable to predict the response variable. The Multiple Linear Regression model can be represented using the equation: \n",
    "\n",
    "    Y = a0 + a1*x1 + a2*x2 +...+ an*xn\n",
    "    \n",
    "    where,\n",
    "    Y= Output/Response variable; \n",
    "    a0, a1, a2, a3 , an....= Coefficients of the model; \n",
    "    x1, x2, x3, x4,...= Various Independent/feature variable\n",
    "\n",
    "    1. For multiple linear regression, the dependent or target variable(Y) must be the continuous/real, but the predictor or independent variable may be of continuous or categorical form.\n",
    "    2. Each feature variable must model the linear relationship with the dependent variable.\n",
    "    3. Multiple linear regression tries to fit a regression line through a multidimensional space of data-points.\n",
    "\n",
    "    It is differ from Simple linear regression as:\n",
    "\n",
    "    Simple linear regression has only one x and one y variable. whereas, Multiple linear regression has one y and two or more x variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f38f0b-b404-453c-a5f7-152d0607776b",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "    Multicollinearity occurs when two or more independent variables in a data frame have a high correlation with one another in a regression model. This means that one independent variable can be predicted from another in a regression model. It makes it hard to interpret of model and also creates an overfitting problem.\n",
    "\n",
    "    To detect and address this issue:\n",
    "    A statistical technique called the variance inflation factor (VIF) can detect and measure the amount of collinearity in a multiple regression model. VIF measures how much the variance of the estimated regression coefficients is inflated as compared to when the predictor variables are not linearly related. A VIF of 1 will mean that the variables are not correlated; a VIF between 1 and 5 shows that variables are moderately correlated, and a VIF between 5 and 10 will mean that variables are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58d5af-cb40-4113-8d4a-abe800148e30",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "    Polynomial Regression is a regression algorithm that models the relationship between a dependent(y) and independent variable(x) as nth degree polynomial. It is a linear model with some modification in order to increase the accuracy. The dataset used in Polynomial regression for training is of non-linear nature. It makes use of a linear regression model to fit the complicated and non-linear functions and datasets.\n",
    "    \n",
    "    The Polynomial Regression equation is given below:\n",
    "\n",
    "    y = β0 + β1*x + β2*x^2 + β3*x^3 + ... + βp*x^p + ε\n",
    "\n",
    "    where y is the dependent variable, x is the independent variable, β0, β1, β2, ..., βp are the coefficients of the polynomial terms, ε is the error term, and p is the degree of the polynomial.\n",
    "\n",
    "    It is different from linear regression as:\n",
    "    It can capture non-linear relationships between the variables. In linear regression, the relationship between the dependent variable and the independent variable is assumed to be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b381e3-b80a-4f5a-af01-f58b786dd0c9",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "    Advantages of polynomial regression compared to linear regression:\n",
    "\n",
    "    1. Captures non-linear relationships: Polynomial regression can capture non-linear relationships between the dependent variable and the independent variable(s), whereas linear regression assumes a linear relationship.\n",
    "\n",
    "    2. Improved accuracy: When the relationship between the variables is non-linear, polynomial regression can provide a better fit to the data than linear regression. This can lead to improved accuracy in predicting the dependent variable.\n",
    "\n",
    "    Disadvantages of polynomial regression compared to linear regression:\n",
    "\n",
    "    1. Overfitting: Polynomial regression models with high degrees of the polynomial can lead to overfitting, where the model fits the noise in the data instead of the underlying relationship. This can result in poor generalization to new data and reduced model interpretability.\n",
    "\n",
    "    2. Complexity: Polynomial regression models can be more complex than linear regression models, especially for high degrees of the polynomial. This can make the model more difficult to interpret and increase the computational complexity of the model.\n",
    "\n",
    "\n",
    "    Polynomial regression is preferred over linear regression when the relationship between the dependent variable and the independent variable(s) is non-linear. Polynomial regression can capture more complex relationships and provide a better fit to the data, leading to improved accuracy in predicting the dependent variable. However, care must be taken to avoid overfitting, and the degree of the polynomial should be carefully chosen to balance the trade-off between model complexity and accuracy. If the relationship between the variables is linear or can be approximated by a linear function, linear regression may be more appropriate and simpler to interpret."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
