{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c154d97-fbc5-40b2-aacd-11c93895c6c5",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "- R-squared, also known as the coefficient of determination, is the proportion of the variation in the dependent variable that is predictable from the independent variable(s). It is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model.  \n",
    "\n",
    "    R-squared values range from 0 to 1, with higher values indicating a better fit of the model to the data. An R-squared value of 1 indicates that the model explains 100% of the variance in the dependent variable, while an R-squared value of 0 indicates that the model does not explain any of the variance in the dependent variable.\n",
    "\n",
    "    R-squared is calculated as:\n",
    "\n",
    "    R^2 = 1 - (SSR / SST)\n",
    "\n",
    "    where, \n",
    "\n",
    "    - R^2 = R-squared\n",
    "    - SSR = sum of squared residuals (sum of the squared differences between the predicted and actual values of the dependent variable),\n",
    "    - SST = total sum of squares (sum of the squared differences between the actual values of the dependent variable and the mean of the dependent variable.)\n",
    "\n",
    "    R-squared is a goodness-of-fit measure for linear regression models. It represents how well the regression model explains observed data. For example, an r-squared of 60% reveals that 60% of the variability observed in the target variable is explained by the regression model. Generally, a higher r-squared indicates more variability is explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416f1f5-eceb-404b-8563-fe0cb68b8edb",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "- Adjusted R2 is a corrected goodness-of-fit (model accuracy) measure for linear models. It identifies the percentage of variance in the target field that is explained by the input or inputs.\n",
    "\n",
    "    The formula of adjusted R-squared is:\n",
    "    \n",
    "    Adjusted R-squared = 1 - [(1 - R-squared)(n - 1)/(n - k - 1)]\n",
    "\n",
    "    where n is the number of observations and k is the number of independent variables in the model.\n",
    "\n",
    "    R-squared tends to optimistically estimate the fit of the linear regression. It always increases as the number of effects are included in the model increases whether a specific effects improve the model or not. Adjusted R2 attempts to correct for this overestimation. Adjusted R2 might decrease if a specific effect does not improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac218d96-e445-4ba6-8648-21cdfbce5b4d",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "- Adjusted R-squared is more appropriate to use than R-squared when comparing regression models that have a different number of independent variables. When multiple regression models are being compared, R^2 always increases as the number of effects are included in the model increases whether a specific effects improve the model or not. Adjusted R-squared, on the other hand, attempts to correct for this overestimation. Adjusted R2 might decrease if a specific effect does not improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf26aa-d629-4e9f-88da-29dd226e744f",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "- Root Mean Squared Error (RMSE): \n",
    "\n",
    "    Root Mean Squared Error is the measure of how well a regression line fits the data points. It measures the average difference between values predicted by a model and the actual values. It provides an estimation of how well the model is able to predict the target value (accuracy). The lower the value of the Root Mean Squared Error, the better the model is. A perfect model (a hypothetic model that would always predict the exact expected value) would have a Root Mean Squared Error value of 0.\n",
    "\n",
    "    Formula : sqrt(Mean squared error)\n",
    "\n",
    "- Mean Squared Error (MSE):\n",
    "\n",
    "    The Mean Squared Error measures how close a regression line is to a set of data points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. The squaring is necessary to remove any negative signs. A larger MSE indicates that the data points are dispersed widely around its central moment (mean), whereas a smaller MSE suggests the opposite. A smaller MSE is preferred because it indicates that your data points are dispersed closely around its central moment (mean).\n",
    "\n",
    "    Formula: MSE formula = (1/n) * Σ(actual – predicted)^2\n",
    "\n",
    "    Where:\n",
    "    - n = number of data,\n",
    "    - Actual = original or observed y-value,\n",
    "    - predicted = y-value from regression.\n",
    "\n",
    "- Mean Absolute Error (MAE):\n",
    "\n",
    "    The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset. \n",
    "    \n",
    "    MAE = (1/n) * Σ |(actual – predicted)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d209b03-daec-4653-9290-6d15b2510a30",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "Advantage and Disadvantage of RMSE, MSE and MAE:\n",
    "\n",
    "- Root Mean Squared Error (RMSE):\n",
    "\n",
    "    Advantage of RMSE:\n",
    "    - Has the same units as the original data, making it easier to interpret.\n",
    "    - Differentiable\n",
    "    \n",
    "    Disadvantage of RMSE:\n",
    "    - Sensitive to outliers.\n",
    "\n",
    "- Mean Squared Error (MSE):\n",
    "\n",
    "    Advantage of MSE:\n",
    "    - Penalizes larger errors more heavily.\n",
    "    - Differentiable.\n",
    "    - It has only one local/global minima.\n",
    "\n",
    "    Disadvantage of MSE:\n",
    "    - Sensitive to outliers.\n",
    "    - Harder to interpret as it is not in the orginal unit.\n",
    "\n",
    "- Mean Absolute Error (MAE):\n",
    "    \n",
    "    Advantage of MAE:\n",
    "    - Easy to interpret and understand.\n",
    "    - Less sensitive to outliers.\n",
    "    \n",
    "    Disadvantage of MAE:\n",
    "    - Convergence usually takes more time.\n",
    "    - Does not take into account the direction of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3b3a7-8889-42ce-927d-442d077ffac4",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "- Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function of a linear regression model. The penalty term is the L1 norm (sum of absolute values) of the model coefficients, multiplied by a tuning parameter λ.\n",
    "\n",
    "    This is another regularized linear regression model, it works by adding a penalty term to the cost function, but it tends to zero out some features’ coefficients, which makes it useful for feature selection. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination. \n",
    "\n",
    "    Formula:  Σᵢ(yᵢ - ȳᵢ)² + λ Σᵢ|βᵢ|\n",
    "\n",
    "    where:\n",
    "    - λ is a hyperparameter\n",
    "    - yᵢ actual point\n",
    "    - ȳᵢ predicted point\n",
    "\n",
    "- Ridge and lasso regression both address multicollinearity in regression models but are different in the type of penalty used. Ridge regression (L2 regularization) shrinks coefficients towards zero, whereas lasso regression (L1 regularization) can force some coefficients to be exactly 0, making it suitable for feature selection.\n",
    "\n",
    "- The choice between Lasso and Ridge regularization depends on the specific problem. If the dataset has many features and we suspect that only a subset of them are relevant to the target variable, Lasso regularization is more appropriate because it can eliminate irrelevant features. However, if all features are potentially relevant, Ridge regularization may be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ccea7-f8f4-43aa-9a21-6f9d50ba2663",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "- Regularized linear models modifies the over-fitted or under-fitted models by adding the penalty equivalent to the cost function that encourages the model to have smaller coefficients. \n",
    "\n",
    "- For example, let's say we have a dataset with 1000 observations and 20 features. We want to train a linear regression model to predict the target variable. We split the dataset into a training set of 800 observations and a test set of 200 observations. We fit a linear regression model to the training set and evaluate its performance on the test set.\n",
    "\n",
    "    Without regularization, the model may try to fit the noise in the training set, resulting in high variance or overfitting. \n",
    "    To prevent overfitting, we can add a penalty term to the cost function, such as the L1(LASSO regression) or L2(Ridge regression) norm of the coefficients, which encourages the model to have smaller coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15335d-622e-4f70-9055-aaf7e9708339",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "- Limitation of regularized linear models are:\n",
    "    1. Regularized linear models may not handle outliers.\n",
    "    2. Regularized linear models are based on linear assumptions, which means they may not capture more complex non-linear relationships in the data.\n",
    "    3. Regularized linear models may not always be as interpretable as other models, especially when feature selection is applied. If a large number of features are removed, it may be difficult to understand the underlying relationships between the features and the target variable.\n",
    "\n",
    "    The choice of model depends on the specific characteristics of the data, the objectives of the analysis, and the available resources. If the regularization is too weak, the model may overfit, while if it is too strong, the model may underfit. So, they may not always be the best choice for every problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37accd1f-0f9a-419f-9778-387c531dfdba",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "- The model B with an MAE of 8 is performing better. This is because the MAE is a more interpretable metric that measures the absolute distance between the predicted values and the actual values, while the RMSE is a more sensitive metric that squares the errors and then takes the square root, which can amplify the effect of large errors.\n",
    "\n",
    "    Yes, there can be limitations to the choice of evaluation metric in regression analysis:\n",
    "    RMSE and MAE assume that the errors are normally distributed, which may not always be the case in practice. In such cases, other evaluation metrics, such as the Mean Absolute Percentage Error (MAPE), may be more appropriate. The choice of evaluation metric can also be influenced by the characteristics of the data, such as the range and distribution of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff408a1-f2fc-4b99-addb-2c8c11125e09",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "\n",
    "- The choice of the better performer between Model A (Ridge regularization) and Model B (Lasso regularization) depends on the specific characteristics of the dataset and the problem being solved. Generally, if the dataset has a large number of features with many of them potentially irrelevant or highly correlated, Lasso regularization can be more effective in reducing the impact of those features and improving the model's generalization performance. In contrast, Ridge regularization can be more useful when dealing with highly multicollinear features and where small coefficients are preferred.\n",
    "\n",
    "    both regularization methods have their trade-offs and limitations. Ridge regularization tends to shrink the coefficients towards zero, but it does not set them exactly to zero. This means that Ridge regression can still include all the features in the model, but their contribution will be reduced based on their correlation with other features. In contrast, Lasso regularization tends to force some coefficients to exactly zero, effectively performing feature selection and reducing the number of variables in the model. However, this can also lead to bias if important features are wrongly excluded from the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
