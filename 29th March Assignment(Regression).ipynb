{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b832540-0880-4826-8e9e-50d5313e75f7",
   "metadata": {},
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso regression is a type of linear regression that is used to select a subset of the most important features for the prediction of a target variable. The word “LASSO” stands for Least Absolute Shrinkage and Selection Operator. It is a statistical formula for the regularisation of data models and feature selection. In lasso regression, the coefficients of the regression equation are estimated by minimizing the sum of squared errors subject to a constraint on the absolute size of the coefficients.\n",
    "\n",
    "Lasso Regression uses L1 regularization technique. It is used when we have more features because it automatically performs feature selection.\n",
    "\n",
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "\n",
    "The difference between lasso regression and other regression techniques, such as ridge regression or ordinary least squares regression, is in the way the coefficients are estimated. Ridge regression, for example, adds a penalty term to the sum of squared errors, which constrains the size of the coefficients but does not necessarily set them to zero. Lasso regression, on the other hand, can set some of the coefficients to zero, effectively eliminating the corresponding features from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c61c2-f7fd-4666-87b3-5bedc255a283",
   "metadata": {},
   "source": [
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "The main advantage of using Lasso Regression for feature selection is that it can select the most important features for the prediction of a target variable.\n",
    "\n",
    "In many real-world applications, there may be a large number of features available for modeling, but only a subset of these features may be relevant for the prediction of the target variable. Identifying the relevant features can be a challenging task, especially when the number of features is large.\n",
    "\n",
    "Lasso Regression can address this problem by automatically setting some of the coefficients (Which do not effect the model) to zero, effectively eliminating the corresponding features from the model. This makes the model simpler and easier to interpret, and can also reduce the risk of overfitting, which can occur when a model is too complex and fits the noise in the data rather than the underlying pattern. Lasso Regression can also improve the accuracy and generalization performance of the model by selecting only important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f9a9e-14b7-4ae5-8c14-1ec9f97c5845",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "In Lasso Regression, the coefficients of the model are estimated by minimizing the sum of squared errors subject to a constraint on the absolute size of the coefficients.\n",
    "\n",
    "Interpretation of Lasso Regression:\n",
    "\n",
    "Non-zero coefficients represent the strength and direction of the relationship between the corresponding feature and the target variable. If a non-zero coefficient for a feature is positive, it indicates that an increase in the value of that feature is associated with an increase in the predicted value of the target variable. Conversely, if a non-zero coefficient is negative, it indicates that an increase in the value of that feature is associated with a decrease in the predicted value of the target variable.\n",
    "\n",
    "The features with zero coefficients are considered to be irrelevant or redundant for the prediction of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ebc7d-98e1-4aa6-b723-1f7484315c4f",
   "metadata": {},
   "source": [
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "There are two main tuning parameters in Lasso Regression that can be adjusted to control the behavior of the model:\n",
    "\n",
    "- Alpha (α): Alpha is a hyperparameter that controls the strength of the L1 penalty applied to the coefficients. The L1 penalty is what makes Lasso Regression a form of regularization, and it helps to reduce the complexity of the model and avoid overfitting. A larger value of α will result in a stronger penalty on the coefficients, which can lead to more coefficients being set to exactly zero and a sparser model. A smaller value of α will result in a weaker penalty and a less sparse model.\n",
    "\n",
    "- Maximum iterations: Lasso Regression is solved using an optimization algorithm, and the maximum number of iterations can be specified to control how long the algorithm runs. If the algorithm reaches the maximum number of iterations without converging to a solution, it will stop and return the best solution found so far. Setting a higher maximum number of iterations can help the model to converge to a better solution, but it can also increase the computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb3053-8da4-4896-9e75-a8655e9471f9",
   "metadata": {},
   "source": [
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Yes, Lasso regularization can be used for non-linear regression problems with Gaussian basis functions. Regularization with a lasso penalty is advantageous in that it estimates some coefficients in linear regression models to be exactly zero. If you can linearize the model, then Lasso can be used for non-linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3b10e-79bf-4212-9d30-67aed986a0c3",
   "metadata": {},
   "source": [
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Difference between Ridge regression and lasso regression are:\n",
    "\n",
    "- In Ridge regression The penality term is the sum of the squares of the coefficients(L2 Regularization).\n",
    "    while, In Lasso regression, The penality term is the sum of the absolute value of the coefficients(L1 Regularization).\n",
    "- Ridge regression shrinks the coefficients but doesn't set any coefficient to zero. While, Lasso regression can shrinks some coefficients to zero, effectively performing feature selection.\n",
    "- Ridge regression helps to reduce overfitting by shrinking large coefficients. While, Lasso regression helps to reduce overfitting by shrinking and selecting features with less importance.\n",
    "- Ridge regression works well when there are a large number of features. While Lasso regression works well there are a small number of features.\n",
    "- Ridge regression performs \"soft thresholding\" of coefficients. While, Lasso regression perfomrs \"hard thresholding\" of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726722a-90c1-4e59-ac8d-95ad5d09c2d0",
   "metadata": {},
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity in the input features. Because the penalty term encourages sparsity in the model, Lasso Regression can effectively handle multicollinearity by automatically selecting a subset of the most important features and setting the coefficients of the others to zero. This helps to reduce the impact of multicollinearity on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff83d8ed-ff08-4ad1-9aa2-5cd6abce06fd",
   "metadata": {},
   "source": [
    "# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "To choose the optimal value of λ we use cross-validation, which involves splitting the data into training and validation sets, and evaluating the model's performance on the validation set for different values of λ. The value of λ that gives the best performance on the validation set is chosen as the optimal value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
